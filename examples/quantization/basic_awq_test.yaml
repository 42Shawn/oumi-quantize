# Basic AWQ Quantization Test Configuration
# Tests AWQ q4_0 method with a small model for quick validation

model:
  model_name: "microsoft/DialoGPT-small"  # Small model for testing
  tokenizer_name: "microsoft/DialoGPT-small"

# AWQ quantization method - 4-bit with good quality
method: "awq_q4_0"

# Output configuration
output_path: "test_outputs/dialogpt_small_awq_q4.gguf"
output_format: "gguf"

# AWQ specific settings
awq_group_size: 128        # Default group size
awq_zero_point: true       # Enable zero point for better accuracy
awq_version: "GEMM"        # Use GEMM kernels
calibration_samples: 128   # Fewer samples for faster testing
cleanup_temp: true         # Clean up temporary files

# General settings
batch_size: 8              # Small batch for memory efficiency
verbose: true              # Enable detailed logging