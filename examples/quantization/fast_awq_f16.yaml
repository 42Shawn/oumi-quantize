# Fast AWQ F16 Configuration
# For fast conversion with AWQ optimizations but minimal compression

model:
  model_name: "codellama/CodeLlama-7b-Python-hf"
  tokenizer_name: "codellama/CodeLlama-7b-Python-hf"
  model_kwargs:
    trust_remote_code: true

# AWQ F16 method for format conversion with optimizations
method: "awq_f16"

# Output configuration
output_path: "models/fast/codellama-7b-python-awq-f16.gguf"
output_format: "gguf"

# Fast AWQ settings
awq_group_size: 256        # Larger groups for speed
awq_zero_point: true       
awq_version: "GEMM"        
calibration_samples: 256   # Fewer samples for speed
cleanup_temp: true         

# Performance settings
batch_size: 32             # Larger batch for F16
verbose: true