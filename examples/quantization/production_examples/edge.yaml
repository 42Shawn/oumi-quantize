# Edge Deployment Quantization Configuration  
# Maximum compression for resource-constrained environments

model:
  model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  tokenizer_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# AWQ 4-bit for maximum compression
method: "awq_q4_0"
output_path: "models/edge/tinyllama-edge.gguf"
output_format: "gguf"

# Edge-optimized AWQ settings
awq_group_size: 128          # Standard group size
calibration_samples: 256     # Fewer samples for speed
awq_zero_point: true         # Better accuracy
awq_version: "GEMM"          # Faster kernels
cleanup_temp: true           # Save disk space

# Memory-efficient settings for edge devices
batch_size: 4                # Small batch for low memory
verbose: false               # Reduce logging overhead

# Expected Results:
# - Original: ~2.2 GB
# - Quantized: ~661 MB
# - Compression: 3.3x
# - Quality: Good for edge deployment
# - Memory usage: < 2GB RAM during quantization