# GPU-Optimized Quantization Configuration
# Optimized for GPU inference with PyTorch/HuggingFace ecosystem

model:
  model_name: "microsoft/DialoGPT-medium"
  tokenizer_name: "microsoft/DialoGPT-medium"
  model_kwargs:
    torch_dtype: "float16"    # GPU-optimized precision
    device_map: "auto"        # Automatic GPU mapping

# BitsAndBytes 4-bit for GPU optimization
method: "bnb_4bit"
output_path: "models/gpu/dialogpt-medium-4bit.pytorch"
output_format: "pytorch"

# GPU-optimized settings
batch_size: 32               # Larger batch for GPU efficiency
verbose: true

# Notes:
# - Works with most GPU setups
# - Compatible with HuggingFace transformers
# - Direct GPU memory optimization
# - Supports models not compatible with AWQ (like GPT-2 family)