# Llama 2 7B AWQ Quantization Example
# This configuration demonstrates AWQ quantization on a production-scale model

# Model configuration
model:
  model_name: "meta-llama/Llama-2-7b-hf"  # Base model (not chat version for this example)
  tokenizer_name: "meta-llama/Llama-2-7b-hf"
  model_kwargs:
    torch_dtype: "auto"          # Let transformers choose optimal dtype
    device_map: "auto"           # Automatic device placement
    trust_remote_code: false     # Security: don't execute remote code

# AWQ quantization settings
method: "awq_q4_0"              # AWQ 4-bit quantization with best quality/size balance
output_path: "models/llama2-7b-awq-q4.gguf"
output_format: "gguf"           # GGUF format for llama.cpp compatibility

# AWQ-specific parameters
awq_group_size: 128             # Standard group size for good accuracy
awq_zero_point: true            # Enable zero-point for better accuracy
awq_version: "GEMM"             # Use GEMM kernels (faster than GEMV)
calibration_samples: 512        # Good balance of quality vs speed
cleanup_temp: true              # Clean up intermediate files to save disk space

# Performance settings
batch_size: 8                   # Conservative batch size for memory efficiency
verbose: true                   # Enable detailed logging

# Optional: Override default calibration settings
# Note: These are advanced settings - defaults work well for most cases
# awq_calib_dataset: "pileval"  # Default calibration dataset
# awq_calib_split: "train"      # Dataset split to use
# awq_calib_text_column: "text" # Text column name in dataset