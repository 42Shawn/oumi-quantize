# Basic Model Quantization Configuration
# ðŸš§ DEVELOPMENT: This configuration tests the quantization interface
# Currently runs in simulation mode - validates inputs without performing actual quantization

# Model configuration
model:
  model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  tokenizer_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  trust_remote_code: false

# Quantization settings
method: "q4_0"                    # 4-bit quantization (good compression/quality balance)
output_path: "tinyllama-1.1b-q4.gguf"  # Output file path
output_format: "gguf"             # GGUF format for llama.cpp compatibility

# Optional settings
batch_size: null                  # Auto-determine batch size
verbose: true                     # Enable detailed loggings
