# Test Real AWQ Quantization on macOS using BitsAndBytes Fallback
# This configuration tests actual quantization using available libraries

model:
  model_name: "microsoft/DialoGPT-small"  # Small model for quick testing
  tokenizer_name: "microsoft/DialoGPT-small"
  model_kwargs:
    torch_dtype: "auto"
    device_map: "auto"

# AWQ method - will be mapped to BitsAndBytes on macOS
method: "awq_q4_0"
output_path: "models/real_test/dialogpt_small_real_awq.gguf"
output_format: "gguf"

# AWQ parameters (informational for BitsAndBytes mapping)
awq_group_size: 128
awq_zero_point: true
awq_version: "GEMM"
calibration_samples: 128  # Reduced for faster testing
cleanup_temp: false  # Keep temp files for inspection

# Performance settings
batch_size: 4  # Small batch for memory efficiency
verbose: true