# Qwen 7B AWQ Quantization Example
# This configuration demonstrates AWQ quantization on Qwen-7B with 1024 calibration samples

# Model configuration
model:
  model_name: "Qwen/Qwen-7B"          # Qwen 7B base model
  tokenizer_name: "Qwen/Qwen-7B"      # Use same tokenizer
  model_kwargs:
    torch_dtype: "auto"               # Let transformers choose optimal dtype
    device_map: "auto"                # Automatic device placement for multi-GPU
    trust_remote_code: true           # Qwen models require remote code execution
    use_cache: false                  # Disable KV cache for quantization

# AWQ quantization settings
method: "awq_q4_0"                    # AWQ 4-bit quantization with best quality/size balance
output_path: "qwen-7b-awq-q4.gguf"   # Output file path
output_format: "gguf"                 # GGUF format for llama.cpp compatibility

# AWQ-specific parameters
awq_group_size: 128                   # Standard group size for good accuracy
awq_zero_point: true                  # Enable zero-point for better accuracy
awq_version: "GEMM"                   # Use GEMM kernels (faster than GEMV)
calibration_samples: 128             # Higher calibration samples for better accuracy
cleanup_temp: true                    # Clean up intermediate files to save disk space

# Performance settings
batch_size: 4                         # Conservative batch size for Qwen-7B
verbose: true                         # Enable detailed logging

# Optional: Custom calibration settings
# You can uncomment and modify these for specific use cases
# awq_calib_dataset: "wikitext"       # Alternative calibration dataset
# awq_calib_split: "train"            # Dataset split to use
# awq_calib_text_column: "text"       # Text column name in dataset