# Llama 2 7B AWQ Quantization Variants
# This file shows different AWQ configurations for different use cases

# =============================================================================
# VARIANT 1: Maximum Quality (AWQ Q8)
# =============================================================================
# Use this for applications where quality is more important than file size
# Expected output: ~7GB, minimal quality loss

# model:
#   model_name: "meta-llama/Llama-2-7b-hf"
#   tokenizer_name: "meta-llama/Llama-2-7b-hf"
# method: "awq_q8_0"
# output_path: "models/llama2-7b-awq-q8-quality.gguf"
# output_format: "gguf"
# awq_group_size: 64              # Smaller groups for better accuracy
# awq_zero_point: true
# awq_version: "GEMM"
# calibration_samples: 1024       # More samples for better calibration
# cleanup_temp: false             # Keep intermediate files for analysis
# batch_size: 4                   # Conservative for memory
# verbose: true

# =============================================================================
# VARIANT 2: Maximum Compression (AWQ Q4_0)
# =============================================================================
# Use this for deployment where file size is critical
# Expected output: ~4GB, good quality/size balance

model:
  model_name: "meta-llama/Llama-2-7b-hf"
  tokenizer_name: "meta-llama/Llama-2-7b-hf"
  model_kwargs:
    torch_dtype: "auto"
    device_map: "auto"

method: "awq_q4_0"               # Default - best balance
output_path: "models/llama2-7b-awq-q4-compact.gguf"
output_format: "gguf"
awq_group_size: 128              # Standard setting
awq_zero_point: true
awq_version: "GEMM"
calibration_samples: 512
cleanup_temp: true
batch_size: 8
verbose: true

# =============================================================================
# VARIANT 3: Enhanced Quality Q4_1
# =============================================================================
# Use this for better 4-bit quality with bias terms
# Expected output: ~4GB, better quality than q4_0

# model:
#   model_name: "meta-llama/Llama-2-7b-hf"
#   tokenizer_name: "meta-llama/Llama-2-7b-hf"
# method: "awq_q4_1"
# output_path: "models/llama2-7b-awq-q4_1-enhanced.gguf"
# output_format: "gguf"
# awq_group_size: 128
# awq_zero_point: true
# awq_version: "GEMM"
# calibration_samples: 512
# cleanup_temp: true
# batch_size: 8
# verbose: true

# =============================================================================
# VARIANT 4: Fast Conversion (AWQ F16)
# =============================================================================
# Use this for format conversion with AWQ optimizations but minimal compression
# Expected output: ~13GB, fastest conversion, format optimized

# model:
#   model_name: "meta-llama/Llama-2-7b-hf"
#   tokenizer_name: "meta-llama/Llama-2-7b-hf"
# method: "awq_f16"
# output_path: "models/llama2-7b-awq-f16-fast.gguf"
# output_format: "gguf"
# awq_group_size: 256             # Larger groups for speed
# awq_zero_point: true
# awq_version: "GEMM"
# calibration_samples: 256        # Fewer samples for speed
# cleanup_temp: true
# batch_size: 16                  # Larger batch for F16
# verbose: true

# =============================================================================
# VARIANT 5: Memory-Constrained Setup
# =============================================================================
# Use this if you have limited RAM/VRAM
# Expected output: ~4GB, optimized for low memory usage

# model:
#   model_name: "meta-llama/Llama-2-7b-hf"
#   tokenizer_name: "meta-llama/Llama-2-7b-hf"
#   model_kwargs:
#     torch_dtype: "float16"      # Explicit half precision
#     device_map: "auto"
#     low_cpu_mem_usage: true     # Reduce CPU memory usage
# method: "awq_q4_0"
# output_path: "models/llama2-7b-awq-q4-lowmem.gguf"
# output_format: "gguf"
# awq_group_size: 128
# awq_zero_point: true
# awq_version: "GEMM"
# calibration_samples: 256        # Fewer samples to reduce memory
# cleanup_temp: true
# batch_size: 2                   # Very small batch
# verbose: false                  # Reduce logging overhead

# =============================================================================
# Instructions:
# =============================================================================
# 1. Uncomment ONE variant above (comment out the current default)
# 2. Run: oumi quantize --config examples/quantization/llama2_awq_variants.yaml
# 3. Compare results and choose the best variant for your use case
#
# Memory Requirements by Variant:
# - Variant 1 (Q8): ~28GB RAM, best quality
# - Variant 2 (Q4_0): ~24GB RAM, balanced (DEFAULT)
# - Variant 3 (Q4_1): ~24GB RAM, enhanced quality
# - Variant 4 (F16): ~32GB RAM, fast conversion
# - Variant 5 (Low-mem): ~16GB RAM, memory optimized