{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div class=\"align-center\">\n<a href=\"https://oumi.ai/\"><img src=\"https://oumi.ai/docs/en/latest/_static/logo/header_logo.png\" height=\"200\"></a>\n\n[![Documentation](https://img.shields.io/badge/Documentation-latest-blue.svg)](https://oumi.ai/docs/en/latest/index.html)\n[![Discord](https://img.shields.io/discord/1286348126797430814?label=Discord)](https://discord.gg/oumi)\n[![GitHub Repo stars](https://img.shields.io/github/stars/oumi-ai/oumi)](https://github.com/oumi-ai/oumi)\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi - Quantization Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n</div>\n\nüëã Welcome to Open Universal Machine Intelligence (Oumi)!\n\nüöÄ Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from [data preparation](https://oumi.ai/docs/en/latest/resources/datasets/datasets.html) and [training](https://oumi.ai/docs/en/latest/user_guides/train/train.html) to [evaluation](https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html) and [deployment](https://oumi.ai/docs/en/latest/user_guides/launch/launch.html). Whether you're developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.\n\nü§ù Make sure to join our [Discord community](https://discord.gg/oumi) to get help, share your experiences, and contribute to the project! If you are interested in joining one of the community's open-science efforts, check out our [open collaboration](https://oumi.ai/community) page.\n\n‚≠ê If you like Oumi and you would like to support it, please give it a star on [GitHub](https://github.com/oumi-ai/oumi).\n\n# Model Quantization Tutorial\n\nThis tutorial demonstrates how to use AWQ (Activation-aware Weight Quantization) to compress large language models while maintaining performance.\n\n## Prerequisites\n\n‚ùó**NOTICE:** Model quantization requires a GPU. If running on Google Colab, you must use a GPU runtime (Colab Menu: `Runtime` -> `Change runtime type` -> Select `T4 GPU` or better).\n\n‚ö†Ô∏è **DEVELOPMENT STATUS**: The quantization feature is currently under active development. Some features may change in future releases.\n\nFirst, let's install Oumi with GPU support and the required quantization libraries:\n\n```bash\npip install oumi[gpu]\npip install autoawq\npip install triton==3.0.0  # Required for AWQ inference compatibility\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic AWQ Quantization\n",
    "\n",
    "Let's start by quantizing TinyLlama to 4-bit using AWQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from oumi.core.configs import QuantizationConfig, ModelParams  # type: ignore\nfrom oumi.quantize import quantize  # type: ignore\n\n# Configure quantization\nconfig = QuantizationConfig(\n    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n    method=\"awq_q4_0\",  # 4-bit AWQ quantization\n    output_path=\"tinyllama_awq_4bit.safetensors\",\n    output_format=\"safetensors\",  # Use SafeTensors format\n    calibration_samples=512,  # Number of calibration samples\n)\n\n# Run quantization\nprint(\"Starting AWQ quantization...\")\nresult = quantize(config)\n\n# Calculate sizes and compression\noriginal_size_gb = 2.2  # TinyLlama 1.1B in fp16\nquantized_size_gb = result[\"quantized_size_bytes\"] / (1024**3)  # type: ignore\ncompression_ratio = original_size_gb / quantized_size_gb\n\nprint(\"\\n‚úÖ Quantization complete!\")\nprint(f\"Original size (fp16): {original_size_gb:.2f}GB\")\nprint(f\"Quantized size (4-bit): {quantized_size_gb:.2f}GB\")\nprint(f\"Compression ratio: {compression_ratio:.1f}x\")\nsize_reduction_pct = (original_size_gb - quantized_size_gb) / original_size_gb * 100\nprint(f\"Size reduction: {size_reduction_pct:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Quantized Model\n",
    "\n",
    "Now let's load and use the quantized model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from awq import AutoAWQForCausalLM  # type: ignore\nfrom oumi.builders import build_tokenizer  # type: ignore\nfrom oumi.core.configs import ModelParams  # type: ignore\nimport torch  # type: ignore\n\n# Load the quantized model\nmodel_path = \"tinyllama_awq_4bit.safetensors\"\n\nprint(f\"Loading AWQ model from: {model_path}\")\nmodel = AutoAWQForCausalLM.from_quantized(\n    model_path,\n    fuse_layers=False,  # Disable layer fusion to avoid compatibility issues\n    device_map=\"auto\",\n)\n\n# Load tokenizer using oumi builder\ntokenizer = build_tokenizer(\n    ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"‚úÖ Model loaded! GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Generating response...\n",
      "Response:\n",
      "Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Model quantization is a technique that can achieve significant speed-up by reducing the amount of storage required to hold model parameters. In the context of deep learning, it reduces the storage needed to represent the weights of a model, which in turn reduces the size of the model and makes it easier to transport, store, and transfer. This helps with data reduction, which is crucial for real-time deployment scenarios. Moreover, model quantization can also lead to better utilization of processing capabilities of the GPU, since it reduces the memory requirements for storing model parameters. This reduces the amount of memory required to store and manipulate the model, and enables the usage of the entire memory capacity of the GPU. Overall, model quantization is an\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "prompt = \"Explain the benefits of model quantization in simple terms:\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Configuration\n",
    "\n",
    "AWQ offers several configuration options for fine-tuning the quantization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced AWQ configuration with more calibration samples\nadvanced_config = QuantizationConfig(\n    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n    method=\"awq_q4_0\",\n    output_path=\"tinyllama_awq_advanced.safetensors\",\n    output_format=\"safetensors\",  # Use SafeTensors format\n    # AWQ-specific parameters\n    calibration_samples=1024,  # More samples for better calibration\n    awq_group_size=128,  # Weight grouping size\n    awq_version=\"GEMM\",  # AWQ kernel version (GEMM is faster)\n    awq_zero_point=True,  # Use zero-point quantization\n)\n\nprint(\"Configuration:\")\nprint(f\"- Output format: {advanced_config.output_format}\")\nprint(f\"- Calibration samples: {advanced_config.calibration_samples}\")\nprint(f\"- Group size: {advanced_config.awq_group_size}\")\nprint(f\"- AWQ version: {advanced_config.awq_version}\")\nprint(f\"- Zero point: {advanced_config.awq_zero_point}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. ‚úÖ Quantize models using AWQ to 4-bit precision\n",
    "2. ‚úÖ Load and use AWQ quantized models for inference\n",
    "3. ‚úÖ Configure AWQ parameters for better quality\n",
    "\n",
    "\n",
    "### Key Benefits of AWQ:\n",
    "- **Memory Efficiency**: ~75% reduction in model size\n",
    "- **Speed**: Faster inference due to reduced memory bandwidth\n",
    "- **Quality**: Minimal performance degradation\n",
    "- **Compatibility**: Works with most transformer models\n",
    "\n",
    "Happy quantizing! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}