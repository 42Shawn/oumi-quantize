{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oumi - Model Quantization Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use AWQ (Activation-aware Weight Quantization) to compress large language models while maintaining performance.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- GPU with CUDA support (required for AWQ)\n",
    "- Oumi installed with GPU support: `pip install oumi[gpu]`\n",
    "- AutoAWQ library: `pip install autoawq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic AWQ Quantization\n",
    "\n",
    "Let's start by quantizing TinyLlama to 4-bit using AWQ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AWQ quantization...\n",
      "[2025-08-01 03:40:43,383][oumi][rank0][pid:2763901][MainThread][INFO]][main.py:52] Starting quantization of model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "[2025-08-01 03:40:43,386][oumi][rank0][pid:2763901][MainThread][INFO]][main.py:53] Quantization method: awq_q4_0\n",
      "[2025-08-01 03:40:43,387][oumi][rank0][pid:2763901][MainThread][INFO]][main.py:54] Output path: tinyllama_awq_4bit.pytorch\n",
      "[2025-08-01 03:40:43,511][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:94] Starting AWQ quantization pipeline...\n",
      "[2025-08-01 03:40:43,512][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:177] Loading model for AWQ quantization: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "[2025-08-01 03:40:43,513][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:180] 📥 Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582e0c02c7e54c8b8b13502693bc6ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-01 03:40:43,768][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:199] 🔧 Configuring AWQ quantization parameters...\n",
      "[2025-08-01 03:40:43,769][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:216] ⚙️  AWQ config: {'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}\n",
      "[2025-08-01 03:40:43,770][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:217] 📊 Using 512 calibration samples\n",
      "[2025-08-01 03:40:43,771][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:218] 🧮 Starting AWQ calibration and quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 2048). Running this sequence through the model will result in indexing errors\n",
      "AWQ: 100%|██████████| 22/22 [05:52<00:00, 16.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-01 03:46:39,020][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:139] PyTorch format requested. Saving AWQ model...\n",
      "[2025-08-01 03:46:40,228][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:147] ✅ AWQ quantization successful! Saved as pytorch format.\n",
      "[2025-08-01 03:46:40,229][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:148] 📊 Final size: 734.0 MB\n",
      "[2025-08-01 03:46:40,230][oumi][rank0][pid:2763901][MainThread][INFO]][awq_quantizer.py:159] 💡 Use this model with: AutoAWQForCausalLM.from_quantized('tinyllama_awq_4bit.pytorch')\n",
      "[2025-08-01 03:46:40,232][oumi][rank0][pid:2763901][MainThread][INFO]][main.py:98] Quantization completed successfully!\n",
      "\n",
      "✅ Quantization complete!\n",
      "Original size (fp16): 2.20GB\n",
      "Quantized size (4-bit): 0.72GB\n",
      "Compression ratio: 3.1x\n",
      "Size reduction: 67.4%\n",
      "\n",
      "🔍 Debug info:\n",
      "Raw quantized size: 769,872,152 bytes\n",
      "Expected 4-bit size: ~0.55GB\n"
     ]
    }
   ],
   "source": [
    "from oumi.core.configs import QuantizationConfig, ModelParams\n",
    "from oumi.quantize import quantize\n",
    "\n",
    "# Configure quantization\n",
    "config = QuantizationConfig(\n",
    "    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n",
    "    method=\"awq_q4_0\",  # 4-bit AWQ quantization\n",
    "    output_path=\"tinyllama_awq_4bit.pytorch\",\n",
    "    output_format=\"pytorch\",\n",
    "    calibration_samples=512,  # Number of calibration samples\n",
    ")\n",
    "\n",
    "# Run quantization\n",
    "print(\"Starting AWQ quantization...\")\n",
    "result = quantize(config)\n",
    "\n",
    "# Calculate sizes and compression\n",
    "original_size_gb = 2.2  # TinyLlama 1.1B in fp16\n",
    "quantized_size_gb = result[\"quantized_size_bytes\"] / (1024**3)  # type: ignore\n",
    "compression_ratio = original_size_gb / quantized_size_gb\n",
    "\n",
    "print(f\"\\n✅ Quantization complete!\")\n",
    "print(f\"Original size (fp16): {original_size_gb:.2f}GB\")\n",
    "print(f\"Quantized size (4-bit): {quantized_size_gb:.2f}GB\")\n",
    "print(f\"Compression ratio: {compression_ratio:.1f}x\")\n",
    "print(\n",
    "    f\"Size reduction: {((original_size_gb - quantized_size_gb) / original_size_gb * 100):.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Quantized Model\n",
    "\n",
    "Now let's load and use the quantized model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AWQ model from: tinyllama_awq_4bit.pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 22/22 [00:02<00:00,  8.15it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e0fc8bfa7c4da7ac8285a72805fefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded! GPU memory: 0.42GB\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the quantized model\n",
    "model_path = \"tinyllama_awq_4bit.pytorch\"\n",
    "\n",
    "print(f\"Loading AWQ model from: {model_path}\")\n",
    "model = AutoAWQForCausalLM.from_quantized(\n",
    "    model_path,\n",
    "    fuse_layers=False,  # Disable layer fusion to avoid compatibility issues\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"✅ Model loaded! GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Generating response...\n",
      "Response:\n",
      "Explain the benefits of model quantization in simple terms:\n",
      "\n",
      "Model quantization is a technique that can achieve significant speed-up by reducing the amount of storage required to hold model parameters. In the context of deep learning, it reduces the storage needed to represent the weights of a model, which in turn reduces the size of the model and makes it easier to transport, store, and transfer. This helps with data reduction, which is crucial for real-time deployment scenarios. Moreover, model quantization can also lead to better utilization of processing capabilities of the GPU, since it reduces the memory requirements for storing model parameters. This reduces the amount of memory required to store and manipulate the model, and enables the usage of the entire memory capacity of the GPU. Overall, model quantization is an\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "prompt = \"Explain the benefits of model quantization in simple terms:\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Configuration\n",
    "\n",
    "AWQ offers several configuration options for fine-tuning the quantization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "- Calibration samples: 1024\n",
      "- Group size: 128\n",
      "- AWQ version: GEMM\n",
      "- Zero point: True\n"
     ]
    }
   ],
   "source": [
    "# Advanced AWQ configuration with more calibration samples\n",
    "advanced_config = QuantizationConfig(\n",
    "    model=ModelParams(model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"),\n",
    "    method=\"awq_q4_0\",\n",
    "    output_path=\"tinyllama_awq_advanced.pytorch\",\n",
    "    # AWQ-specific parameters\n",
    "    calibration_samples=1024,  # More samples for better calibration\n",
    "    awq_group_size=128,  # Weight grouping size\n",
    "    awq_version=\"GEMM\",  # AWQ kernel version (GEMM is faster)\n",
    "    awq_zero_point=True,  # Use zero-point quantization\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"- Calibration samples: {advanced_config.calibration_samples}\")\n",
    "print(f\"- Group size: {advanced_config.awq_group_size}\")\n",
    "print(f\"- AWQ version: {advanced_config.awq_version}\")\n",
    "print(f\"- Zero point: {advanced_config.awq_zero_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "1. ✅ Quantize models using AWQ to 4-bit precision\n",
    "2. ✅ Load and use AWQ quantized models for inference\n",
    "3. ✅ Configure AWQ parameters for better quality\n",
    "\n",
    "\n",
    "### Key Benefits of AWQ:\n",
    "- **Memory Efficiency**: ~75% reduction in model size\n",
    "- **Speed**: Faster inference due to reduced memory bandwidth\n",
    "- **Quality**: Minimal performance degradation\n",
    "- **Compatibility**: Works with most transformer models\n",
    "\n",
    "Happy quantizing! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oumi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
